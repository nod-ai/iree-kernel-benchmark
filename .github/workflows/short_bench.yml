name: Short Benchmark

on:
  workflow_dispatch:
    inputs:
      machine:
        type: choice
        description: Which machine to run benchmarks on
        options:
          - mi325
          - mi355
        default: mi325
      selected_backend:
        description: "Which backends to run benchmarks for (wave, iree, torch, hipblaslt, all)"
        required: false
        default: "all"
      selected_kernel:
        description: "Which kernels to run benchmarks for (conv, gemm, attention, all)"
        required: false
        default: "all"
      problems_url:
        description: "URL to problems to benchmark"
        required: true
      tuned_config_url:
        description: "URL to tuning configurations"
        required: false
      metadata:
        description: "Optional: metadata string for tagging the result"
        required: false
      pr_branch:
        description: "Branch name to check out (e.g., feature/opt-pass)"
        required: false
      pr_repository:
        description: "Repository to check out from (e.g., iree-org/wave)"
        required: false
      pr_headsha:
        description: "Head sha of pull request that triggered dispatch"
        required: false

concurrency:
  group: ${{ github.workflow }}-${{ inputs.pr_headsha }}-${{ inputs.machine }}
  cancel-in-progress: true

jobs:
  identifier:
    name: Benchmark Run Identifier
    runs-on: ubuntu-latest
    steps:
      - name: headSha_${{ inputs.pr_headsha || 'undefined' }}
        run: echo run identifier ${{ inputs.pr_headsha || 'undefined' }}

  short_benchmark:
    name: Short Benchmark
    needs: identifier
    runs-on: "linux-${{ inputs.machine }}-1gpu-ossci-nod-ai"

    container:
      image: rocm/pytorch:rocm7.0.2_ubuntu24.04_py3.12_pytorch_release_2.8.0
      options: >-
        --network=host
        --ipc=host
        --device=/dev/kfd
        --device=/dev/dri
        --group-add video
        --cap-add=SYS_PTRACE
        --security-opt seccomp=unconfined

    concurrency:
      group: kernel-benchmarking-${{ inputs.machine }}
      cancel-in-progress: false

    env:
      WAVE_CACHE_ON: 0

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: "Setting up Rust"
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable

      - name: Setup venv
        run: |
          chmod +x ./setup.sh
          ./setup.sh --no-install-torch --wave-repo ${{ inputs.pr_repository }} --wave-branch ${{ inputs.pr_branch }}

      - name: Load Problems from URL
        run: |
          curl -o problems.json ${{ inputs.problems_url }}
          python -m json.tool problems.json > /dev/null || { echo "Error: Invalid JSON format"; exit 1; }

      - name: Load Tuning Configuration from URL
        if: ${{ inputs.tuned_config_url }}
        run: |
          curl -o tuned_config.json ${{ inputs.tuned_config_url }}
          python -m json.tool tuned_config.json > /dev/null || { echo "Error: Invalid JSON format"; exit 1; }

      - name: Run Benchmarks
        run: |
          python3 -m kernel_bench.cli.bench --backend=${{ inputs.selected_backend }} --kernel_type=${{ inputs.selected_kernel }} --load_problems=problems.json --use_tuned=tuned_config.json --machine=${{ inputs.machine }}x

      - name: Tag result
        if: ${{ inputs.metadata }}
        run: |
          mkdir -p ./results
          cat <<'EOF' > ./results/metadata.txt
          "${{ inputs.metadata }}"

      - name: Upload benchmark results
        uses: actions/upload-artifact@4cec3d8aa04e39d1a68397de0c4cd6fb9dce8ec1
        with:
          name: benchmark-results
          path: ./results/json/
